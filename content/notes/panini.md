---
title : The Pāṇinian Approach to World Modelling
date : 11/02/2025
---
> "the simplest of several competing explanations is likely to be the correct one" - Occam's Razor

## Intelligence as compression

Let’s examine a modified version of Searle’s Chinese room experiment. Suppose we have 5.6TB of text data, ethically scraped from the internet, broken into word pairs (2-grams) stored in a lookup table. When asked to complete "I was going to wear a...", it might meaninglessly output "a lot" because "a lot" appears more frequently than "a shirt" or "a skirt". A 3-gram model, using two words of context, improves accuracy but still fails in cases like "It’s raining outside, wear a...". Both are examples of generative models: they predict the statistically-most-likely next word based on patterns in the data.

Large Language Models (LLMs) like DeepSeek, GPT, or Claude are more sophisticated versions of such generative models. They use thousands of tokens of context and the 'attention' mechanism to focus on relevant parts of the input to 'query' with.  They're not just storing a memorized table of what comes after what, but using compute to extract and store a hiearchy of reusable chunks of information in it's layers. A result is that the weights of the Llama-65B model occupy around 365GB on disk, down from the 5.6TB it's trained on (a 14x compression)[1]. We can see that generalization ability and data efficiency are equivalent: generalization comes from squeezing every bit of information out of your datapoints, understanding all correlations and causations, and connecting all the dots. “Squeezing every bit of information” is meant literally: generalization is the very direct result of compression. 

Yet, LLMs are still many order-of-magnitudes less data-efficient than humans. Lee Sedol, a top Go player, played around 10,000 games in his lifetime, while DeepMind's bot AlphaGo required 30 million games to match him. If Sedol had played 30 million games, how skilled would he be? What would a human who has absorbed all of human knowledge look like? How does the human generative model compress information so effectively? The answer to these question is the key to building machines that think, learn, adapt to tasks like (or better than) humans do, i.e. general machine intelligence.

But it's hard to run this experiment because most humans have seen orders of magnitude less data in their lifetimes, and it's hard to manually inspect human priors. Or so I thought, until I attended the lecture series[2] by Dr. Saroja Bhate at Bangalore International Centre, on Pāṇini, the ancient Sanskrit grammarian. Over 2300 years ago, before the advent of computers or formal logic, Pāṇini sat down and methodically reduced all of human knowledge, then floating around in spoken Vedic Sanskrit, into a generative grammar of exactly 3,995 *sūtras*, or rewrite rules - recorded in his magnum opus, the *Aṣṭādhyāyī*. Not a single rule has been added, removed, or modified since then.

There is a small pool of postdoctoral Sanskrit scholars who are fortunate to understand the full magnitude of Pāṇini's achievement, but I will try to do some justice to it. Faced with a large corpus of spoken Vedic and contemporary Sanskrit, many tens of thousands of hours of audio signals collected without any substrate to record with or automated tooling to work with, Pāṇini, over 12 years, found abstracted atoms of meaning that when combined with a set of dynamic rules and meta-rules formed a generative grammar, a deterministic state machine that could re-synthesizes the original audio corpus - and could be recited in just 2 hours (a >5000x compression!).  

> "If the universe is generated by an algorithm, then observations of that universe, encoded as a dataset, are best predicted by the smallest executable archive of that dataset" - Ray Solomonoff

Pāṇini's foresight in designing these rules means they have stood the test of time - not only expressive enough to explain the knowledge of the past at the time, but have also been able to generalize to phrases and sentences in the future since then. His work is recognized as the first formal system known to man, doing to linguistic reality what Euler would go on to do for geometry, but it would be no understatement to call it the most impressive human act of knowledge compression till date. It is the only example we have access to of true optimal Solomonoff induction, finding the shortest executable archive of a dataset, as evidenced by their durability over 2300 years.

Of course, Pāṇini was working with Sanskrit, whose underlying structure makes it less context-sensitive than English and more amenable to such decomposition. But for the sake of comparison, his methods if automated and applied to the [Hutter compression prize](http://prize.hutter1.net/) dataset could compress 1GB of Wikipedia data to a few kilobytes (down from the current record of 110MB as of Feb 2025). A digital superintelligence in action, would very much employ what I call "Pāṇini's razor", applying the dual techniques of abstraction and economy ruthlessly to thousands of noisy signals of various forms and fidelity, reducing them by many orders of magnitude into succinct set of formal predicates - to derive an explanation unchanging in time. Like breaking down a house into basic lego-like blocks and then building a new house from it back gain, a machine like this could then combine these discovered reusable concepts on the fly using abstracted transformation rules to generalize to any unknown. 

This makes clear the message on the wall - to build powerful thinking machines of the future that can compress information and generalize better than humans do, we must dig up the forgotten relic that is Pāṇini's razor, and study it in great detail. 

## Pāṇini's Razor

[to be continued...]

[1] [Youtube video](https://www.youtube.com/watch?v=dO4TPJkeaaU), Compression for AGI, Jack Rae, Stanford MLSys, ex-OpenAI, 2023

[2] [Youtube Playlist](https://www.youtube.com/playlist?list=PLsAPTmdVuspykLNnjs1_zQKRMqRRfDr2R), Pāṇini Lecture Series, Dr. Saroja Bhate, Bangalore International Center, 2023

